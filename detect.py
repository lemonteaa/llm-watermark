import argparse

import numpy as np
import hashlib

from tokenizers import Tokenizer

from typing import NamedTuple

import json

# copy from generate.py - not all fields needed in detection phase
class WaterMarkConfig(NamedTuple):
    temperature: float
    key: str
    n_vocab: int
    window_m: int
    holdout: int
    ep: float = 0.000001

def new_hash_custom(lst, key):
    def radix_expand(n, base):
        digits = []
        while n > 0:
            digits.append(n % base)
            n = n // base
        return digits
    def expand_list(lst, base):
        res = list()
        for item in lst:
            res += radix_expand(item, base)
        return res
    m = hashlib.sha256()
    history = bytes(expand_list(lst, 256))
    m.update(history)
    m.update(key.encode("utf-8"))
    return m.digest()

def compute_watermark_statistics(tokens, config: WaterMarkConfig):
    elems = []
    
    for idx, tok in enumerate(tokens):
        if idx < config.window_m:
            continue
        hist = o.ids[idx-config.window_m:idx]
        digest = new_hash_custom(hist, config.key)
        rng = np.random.default_rng(seed=[x for x in digest])
        u = rng.random(config.n_vocab).astype(dtype=np.float32)
        r = np.clip(u, config.ep, 1-config.ep)
        elems.append(1.0 - r[tok])
    
    scores = -1.0 * np.ma.log(np.array(elems, dtype=np.float32))
    return scores

# Copy from https://stackoverflow.com/questions/15450192/fastest-way-to-compute-entropy-in-python#45091961
def estimate_entropy(tokens, base=np.e):
    n_labels = len(tokens)
    value,counts = np.unique(tokens, return_counts=True)
    probs = counts / n_labels
    n_classes = np.count_nonzero(probs)
    ent = 0
    for p in probs:
        ent -= p * np.log(p)
    ent = ent / np.log(base)
    return ent

if __name__ == '__main__':
    main_desc = "Educational demostration of the classical Gumbell trick based LLM Watermarking method. This will detect if the text is likely generated by our watermarked LLM. (Warning: be aware of its limitation and DO NOT blindly trust its result)"
    parser = argparse.ArgumentParser(description=main_desc)
    
    # Global arguments    
    global_watermark_options = parser.add_argument_group(title="Watermarking Options", description="You MUST supply the exact same param as when you run the watermarked LLM to get correct result")
    global_watermark_options.add_argument("-s", "--secret", required=True, help="Secret key for the watermarking algorithm")
    global_watermark_options.add_argument("-w", "--window", required=True, type=int, help="Window length for watermarking")
    #global_watermark_options.add_argument("-H", "--holdout", required=True, type=int, help="Generate at least this many tokens before beginning watermarking")
    
    global_analysis_options = parser.add_argument_group(title="Analysis Options")
    global_analysis_options.add_argument("--hf-tok", required=True, help="ID of a Huggingface repo (Format: <user/org handle>/<repo name>) containing the tokenizer of the LLM model.")
    global_analysis_options.add_argument("-f", "--file", help="Source of text to be analyzed. If None, will read from stdin, otherwise read from the file specified.")
    global_analysis_options.add_argument("-o", "--out-np", help="(Optional) If specified, will serialize all relevant numpy data to files with your specified prefix for filename, for your further analysis, such as loading it in a Jupyter Notebook for visualization.")
    
    args = parser.parse_args()
    
    # "Xenova/Meta-Llama-3.1-Tokenizer"
    t = Tokenizer.from_pretrained(args.hf_tok)
    
    file_name = args.file
    
    with open(file_name, mode="r", encoding="utf-8") as f:
        suspect = f.read()
    
    o = t.encode(suspect)
    #o.ids
    
    nv = t.get_vocab_size()
    #128256
    
    conf = WaterMarkConfig(temperature=0.0, key=args.secret, n_vocab=nv, window_m=args.window, holdout=0)
    
    scores = compute_watermark_statistics(o.ids, conf)
    avg_s = np.ma.average(scores)
    print(f"Average score is: {avg_s}")
    v_s = np.ma.var(scores)
    print(f"Estimated variance: {v_s}")
    ent = estimate_entropy(o.ids)
    print(f"Estimated entropy: {ent}")
    # Save file
    if args.out_np is not None:
        np.savez(args.out_np + ".npz", token_ids=o.ids, scores=scores.filled(fill_value=np.nan), average=avg_s, variance=v_s, entropy=ent)
        # Also save text tokenized
        tokenized = o.tokens
        with open(args.out_np + ".json", 'w', encoding='utf-8') as f:
            json.dump(tokenized, f, ensure_ascii=False)
